#!/bin/bash
# llama.cpp ROCm Server Deployment Configuration
# Edit this file to customize your deployment

#######################################
# HARDWARE CONFIGURATION
#######################################
CPU_ARCH="znver4"                    # Zen 4 for Ryzen 7 8845HS
GPU_TARGET="gfx1103"                 # RDNA 3 for Radeon 780M
HSA_GFX_VERSION="11.0.3"            # ROCm HSA version for RDNA 3

#######################################
# ROCM CONFIGURATION
#######################################
ROCM_VERSION="6.2.4"                # ROCm version to install
UBUNTU_CODENAME="noble"             # Ubuntu 24.04 codename

#######################################
# MODEL CONFIGURATION
#######################################
MODEL_NAME="Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf"
MODEL_URL="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/resolve/main/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf"

# Alternative models (comment/uncomment as needed)
# MODEL_NAME="qwen2.5-coder-14b-instruct-q4_k_m.gguf"
# MODEL_URL="https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF/resolve/main/qwen2.5-coder-14b-instruct-q4_k_m.gguf"

#######################################
# SERVER CONFIGURATION
#######################################
INSTALL_DIR="/opt/llama-server"
SERVER_HOST="127.0.0.1"
SERVER_PORT="8091"
SERVER_THREADS="16"                   # CPU threads for processing
GPU_LAYERS="33"                      # -1 for auto, or specific number
CONTEXT_SIZE="8192"                  # Context window size
PARALLEL_REQUESTS="3"                # Number of concurrent users
MAX_MEMORY="48G"                     # Maximum memory allocation

#######################################
# NGINX CONFIGURATION
#######################################
NGINX_PORT="8443"
NGINX_SERVER_NAME="localhost"

# User accounts (username:password format)
# Will be prompted during deployment for security
declare -a USERS=("user1" "user2" "user3")

#######################################
# CLOUDFLARE TUNNEL CONFIGURATION
#######################################
TUNNEL_NAME="llama-server"
# TUNNEL_HOSTNAME will be prompted during setup
# Example: llama.yourdomain.com

#######################################
# DEPLOYMENT OPTIONS
#######################################
SKIP_ROCM_INSTALL=false             # Set to true if ROCm already installed
SKIP_MODEL_DOWNLOAD=false           # Set to true if model already downloaded
SETUP_NGINX=true                    # Set to false to skip nginx setup
SETUP_CLOUDFLARE=false              # Set to true to setup Cloudflare Tunnel
ENABLE_MONITORING=true              # Set to true to install monitoring tools

#######################################
# ADVANCED CONFIGURATION
#######################################
# Rate limiting (requests per second)
RATE_LIMIT="10r/s"
RATE_LIMIT_BURST="20"

# Logging
LOG_DIR="${INSTALL_DIR}/logs"
LOG_LEVEL="info"                    # debug, info, warning, error

# Build options
BUILD_JOBS=$(nproc)                 # Parallel build jobs
BUILD_TYPE="Release"                # Release or Debug
